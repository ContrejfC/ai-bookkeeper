"""
Report Renderer - Convert JSON report to Markdown
=================================================

Usage:
    python ops/reader_eval/report_render.py out/reader_eval/*/report.json > report.md
"""

import argparse
import json
from pathlib import Path


def render_markdown(report: dict) -> str:
    """Render report as Markdown."""
    lines = []
    
    # Header
    lines.append("# Reader Evaluation Report")
    lines.append("")
    lines.append(f"**Timestamp:** {report['timestamp']}")
    lines.append(f"**Duration:** {report['duration_seconds']:.2f}s")
    lines.append("")
    
    # Summary
    summary = report['summary']
    lines.append("## Summary")
    lines.append("")
    lines.append(f"- **Total Datasets:** {summary['datasets']}")
    lines.append(f"- ✅ **Passed:** {summary['pass']}")
    lines.append(f"- ❌ **Failed:** {summary['fail']}")
    lines.append(f"- ⏭️  **Skipped:** {summary['skip']}")
    lines.append("")
    
    pass_rate = (summary['pass'] / summary['datasets'] * 100) if summary['datasets'] > 0 else 0
    lines.append(f"**Overall Pass Rate:** {pass_rate:.1f}%")
    lines.append("")
    
    # Metrics
    metrics = report['metrics']
    lines.append("## Aggregate Metrics")
    lines.append("")
    lines.append("| Metric | Value |")
    lines.append("|--------|-------|")
    lines.append(f"| Reconciliation Pass Rate | {metrics['reconcile_pass_rate']:.1%} |")
    lines.append(f"| Date Parse Rate | {metrics['date_parse_rate']:.1%} |")
    lines.append(f"| Currency Detection Rate | {metrics['currency_detect_rate']:.1%} |")
    lines.append(f"| Low Confidence Fraction | {metrics['low_conf_fraction']:.1%} |")
    lines.append(f"| Deduplication Rate | {metrics['dedup_rate']:.1%} |")
    lines.append("")
    
    # Per-dataset results
    lines.append("## Dataset Results")
    lines.append("")
    
    for dataset in report['datasets']:
        status_icon = {
            "PASS": "✅",
            "FAIL": "❌",
            "SKIP": "⏭️"
        }.get(dataset['status'], "❓")
        
        lines.append(f"### {status_icon} {dataset['name']} ({dataset['kind']})")
        lines.append("")
        lines.append(f"**Status:** {dataset['status']} | **Score:** {dataset['score']:.2f}")
        lines.append("")
        
        # Checks
        if dataset.get('checks'):
            lines.append("**Checks:**")
            checks = dataset['checks']
            for check_name, passed in checks.items():
                check_icon = "✅" if passed else "❌"
                lines.append(f"- {check_icon} {check_name.capitalize()}")
            lines.append("")
        
        # Details
        if dataset.get('details'):
            details = dataset['details']
            lines.append("**Details:**")
            
            if 'rows_expected' in details:
                lines.append(f"- Expected rows: {details['rows_expected']}")
            if 'rows_actual' in details:
                lines.append(f"- Actual rows: {details['rows_actual']}")
            if 'median_confidence' in details:
                lines.append(f"- Median confidence: {details['median_confidence']:.2f}")
            if 'duplicate_count' in details:
                lines.append(f"- Duplicates: {details['duplicate_count']}")
            if 'error_message' in details:
                lines.append(f"- ⚠️  Error: {details['error_message']}")
            
            lines.append("")
    
    # Footer
    lines.append("---")
    lines.append("")
    lines.append("*Generated by Reader Evaluation Harness*")
    lines.append("")
    
    return "\n".join(lines)


def main():
    """CLI entry point."""
    parser = argparse.ArgumentParser(description='Render evaluation report as Markdown')
    parser.add_argument('report_json', type=Path, help='Path to report.json')
    
    args = parser.parse_args()
    
    # Load report
    with open(args.report_json, 'r') as f:
        report = json.load(f)
    
    # Render
    markdown = render_markdown(report)
    
    # Output
    print(markdown)


if __name__ == '__main__':
    main()



